<h2>Домашнее задание по теме “Извлечение информации”</h2>

<h3>Цель: Получить навыки решения классических задач NLP</h3>

<h4>Задание 1: Составление словарей для классификации по тональности</h4>

При классификации текстов или предложений по тональности необходимо использовать оценочные словари для предметной области, то есть, такие словари, в которых содержатся отрицательные и позитивные слова для какой-то предметной области. Идея подобных словарей основана на следующих наблюдениях: во-первых, для разных товаров используются разные оценочные слова (например бывает “захватывающая книга”, но не бывает “захватывающих лыж”), во-вторых, в контексте разных товаров одни и те же слова могут иметь разную окраску (слово “тормоз” в отзыве на велосипед имеет нейтральную окраску, в отзыве на компьютер – резко негативную, “пыль” в контексте пылесосов – нейтральную, в контексте кофемолок – положительную (“мелкий помол в пыль”)). Еще один пример: "теплое пиво" – это плохо, а "теплый свитер" – это хорошо.

Составление таких словарей вручную – трудоемкий процесс, но, к счастью, его не сложно автоматизировать, если собрать достаточно большие корпуса отзывов. В этом домашнем задании вам предстоит попробовать реализовать один их подходов к составлению оценочных словарей, основанный на статье Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora (https://nlp.stanford.edu/pubs/hamilton2016inducing.pdf).

Данные для задания – уже знакомые вам отзывы на банки, собранные с нескольких сайтов Рунета. Отзывы могут быть как положительными (оценка 5), так и отрицательными (оценка 1).

1. Разбейте всю коллекцию отзывов на предложения. Лемматизируйте все слова.
   Обучите по коллекции предложений word2vec
2. Приведите несколько удачных и неудачных примеров решения стандартных текстов для word2vec:
  - тест на определение ближайших слов
  - тест на аналогии (мужчина – король : женщина – королева)
  - тест на определение лишнего слова.
3. Постройте несколько визуализаций:
  - TSNE для топ-100 (или топ-500) слов и найдите осмысленные кластеры слов
  - задайте координаты для нового пространства следующим образом: одна ось описывает отношение "плохо – хорошо", вторая – "медленно – быстро" и найдите координаты названий банков в этих координатах. Более формально: берем вектор слова "хорошо", вычитаем из него вектор слова "плохо", получаем новый вектор, который описывает разницу между хорошими и плохими словами. Берем вектор слова "сбербанк" и умножаем его на этот новый вектор – получаем координату по первой оси. Аналогично – для второй оси. Две координаты уже можно нарисовать на плоскости.


<h4>Задание 2:  Распространение метки</h4>
Определите 5-8 позитивных слов (например, “быстрый”, “удобный”) и 5-8 негативных слов (например,“очередь”, “медленно”). Эти слова будут основной будущего оценочного словаря. Пусть позитивному классу соответствует метка 1, негативному – -1. Пометьте выбранные слова в лексическом графе соответствующими метками. Запустите любой известный вам метод распространения метки (Label Propogation) в лексическом графе. На выходе метода распространения ошибки должны быть новые слова, помеченные метками 1 и -1 – это и есть искомые оценочные слова.

Алгоритмы распространения метки устроены примерно так: пусть мы находимся в выршине, помеченном +1. С какой-то вероятностью мы переносим эту метку на соседние узлы. С меньшей вероятностью переносим ее на вершины на расстоянии два. В конце распространения метки, часть вершин оказывается помечена меткой +1, часть – -1, большая часть остается без метки.

Рекомендуемые алгоритмы распространения метки:

graphlab.label_propagation (graphlab доступен бесплатно по образовательной лицензии)
sklearn.semi_supervised.LabelPropagation
sklearn.semi_supervised.LabelSpreading

<b>Результаты:</b>
Jupyter notebook с предобработанным текстом, аналитикой и выбранными топ-5-8 негативными и позитивными словами

<b>Инструменты:</b>
- Jupyter Notebook 
- Python 
- NLTK
- Gensim 
- Scikit-learn
- Примеры визуализаций: <a href="https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b">Game of Thrones Word Embeddings</a>
- Подход к составлению оценочных словарей: <a href="https://nlp.stanford.edu/pubs/hamilton2016inducing.pdf">Inducing Domain-Specific Sentiment Lexicons</a>

<b>Критерии успешного выполнения</b>: Сделаны все пункты задания.

